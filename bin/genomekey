#!/usr/bin/env python
import argparse
import os
import sys

from cosmos.api import add_execution_args, Execution
from genomekey.workflows.germline.recipe import run_germline
from genomekey.util.misc import assert_references_exist
from genomekey.api import GenomeKey, settings as s, library_path, s3run, s3cmd
from genomekey.aws.config import print_env_export
from genomekey import configuration

opj = os.path.join


# def do_growl(msg, hostname=os.environ.get('SSH_CLIENT', '').split(' ')[0]):
#     from cosmos.util import growl
#
#     growl.send(msg, hostname=hostname)
#
#
# def text_on_finish(ex):
#     from cosmos import signal_execution_status_change, ExecutionStatus
#
#     @signal_execution_status_change.connect
#     def sig(ex):
#         msg = "%s %s" % (ex, ex.status)
#         if ex.status in [ExecutionStatus.successful, ExecutionStatus.failed]:
#             text_message(msg)
#             ex.log.info('Sent a text message')
#
#     def text_message(message):
#         from twilio.rest import TwilioRestClient
#
#         account = s['gk']['twilio_account']
#         token = s['gk']['twilio_token']
#         phone_number = "+1%s" % s['gk']['phone_number']
#         client = TwilioRestClient(account, token)
#
#         message = client.messages.create(to=phone_number, from_='+15203086165', body=message)


def parse_args():
    genomekey = GenomeKey()
    cosmos = genomekey.cosmos_app

    parser = argparse.ArgumentParser()
    parser.add_argument('--ipdb', '-d',
                        help='Launch ipdb on exception',
                        action='store_true')

    sps = parser.add_subparsers(title="Commands", metavar="<command>", help='commands')

    sp = sps.add_parser('resetdb', help=cosmos.resetdb.__doc__)
    sp.set_defaults(func=cosmos.resetdb)

    sp = sps.add_parser('aws', help='print export commands for AWS creentials')
    sp.set_defaults(func=print_env_export)

    sp = sps.add_parser('initdb', help=cosmos.initdb.__doc__)
    sp.set_defaults(func=cosmos.initdb)

    sp = sps.add_parser('shell', help=cosmos.shell.__doc__)
    sp.set_defaults(func=cosmos.shell)

    sp = sps.add_parser('runweb', help=cosmos.runweb.__doc__)
    sp.add_argument('-p', '--port', type=int, default=5000, help='port to bind the server to')
    sp.add_argument('-H', '--host', default='0.0.0.0', help='host to bind the server to')
    sp.set_defaults(func=cosmos.runweb)


    def add_execution_args2(sp):
        add_execution_args(sp)
        sp.add_argument('-m', '--comments', help="adds comments to the execution's info field")
        sp.add_argument('-drm', '--default_drm', help="overrides config value")
        # sp.add_argument('-s', '--sms', help='send an sms after an execution finishes with its status',
        #                 action='store_true')
        # sp.add_argument('-g', '--growl', help='sends growl notifications on execution status changes',
        #                 action='store_true')


    sp = sps.add_parser('germline', help=run_germline.__doc__)
    sp.set_defaults(func=run_germline)
    sp.add_argument('input_path', help='file with inputs')
    sp.add_argument('--config_path', help='genomekey.conf')
    sp.add_argument('--s3fs', '-s3', default=False,
                    help='EXPERIMENTAL.  upload/download all output files to this s3 bucket.  Removes the need'
                         'for a shared filesystem')
    sp.add_argument('--target_bed', '-t', required=True)  # default to whole genome target bed?
    add_execution_args2(sp)

    # sp = sps.add_parser('growl', help='send a message over growl')
    # sp.add_argument('msg', help='message to send')
    # sp.add_argument('-H', '--hostname')
    # sp.set_defaults(func=do_growl)

    args = parser.parse_args()
    kwargs = dict(args._get_kwargs())
    f = kwargs.pop('func')

    workflow_fxns = dict(run_germline='Germline')


    # Todo refactor this into functions
    if f.__name__ in workflow_fxns.keys():
        # assert_references_exist()
        d = {n: kwargs.pop(n) for n in ['name', 'restart', 'skip_confirm', 'max_cores', 'max_attempts', 'comments']}
        comments = d.pop('comments')
        workflow_type = workflow_fxns[f.__name__]

        def get_output_dir():
            root_output_dir = opj(s['gk']['analysis_output'], workflow_type)
            os.system('mkdir -p %s' % root_output_dir)
            return opj(root_output_dir, d['name'])

        d['output_dir'] = get_output_dir()

        resuming_execution = cosmos.session.query(Execution).filter_by(name=d['name']).count() == 1

        ex = cosmos.start(**d)

        ex.info['workflow_type'] = workflow_type
        ex.info['comments'] = comments
        kwargs['execution'] = ex

        s3fs = kwargs.get('s3fs', None)
        if s3fs:
            # AWS cli will produce the following error sometimes if this environment variable is not set
            # [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
            os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'

            s3fs = os.path.join(s3fs, d['name'])
            kwargs['s3fs'] = s3fs
            ex.info['s3fs'] = s3fs

            dir_exists = s3run.path_exists(s3fs)
            if dir_exists:
                if not resuming_execution:
                    ex.log.error('Cannot write to %s, the directory already exists but Execution with name `%s` was not in the database.  '
                                 'Either use a different execution name, or run: \n`aws s3 rm %s --recursive`' % (s3fs, d['name'], s3fs))
                    ex.terminate(due_to_failure=False)
                    ex.delete(delete_files=True)
                    sys.exit(0)
                elif d['restart']:
                    ex.log.info('Deleting S3 Objects in %s since restart==True' % s3fs)
                    s3run.delete_s3_dir(s3fs, skip_confirm=d['skip_confirm'])

        config_path = kwargs.pop('config_path')
        if config_path:
            configuration.load_settings(config_path)


        # required for gof3r, which doesn't check ~/.aws/config files
        # gof3r is used to stream input files from s3 for some tools
        from genomekey.aws.config import set_env_aws_credentials

        set_env_aws_credentials()


    # Send growl on fail
    if kwargs.pop('growl', False):
        from cosmos import signal_execution_status_change, ExecutionStatus

        @signal_execution_status_change.connect
        def growl_signal(execution):
            if execution.status not in [ExecutionStatus.running, ExecutionStatus.killed]:
                do_growl('%s %s' % (execution, execution.status))

    # Send a text on execution end
    if kwargs.pop('sms', False):
        text_on_finish(ex)


    # overwrite default_drm in conf?
    default_drm = kwargs.pop('default_drm', None)
    if default_drm:
        cosmos.default_drm = default_drm

    # launch ipdb on exception if --ipdb
    if kwargs.pop('ipdb', False):
        import ipdb

        with ipdb.launch_ipdb_on_exception():
            f(**kwargs)
    else:
        f(**kwargs)

    if 'execution' in kwargs:
        exit_code = 0 if kwargs['execution'].successful else 1
        sys.exit(exit_code)


if __name__ == '__main__':
    parse_args()