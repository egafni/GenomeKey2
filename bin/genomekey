#!/usr/bin/env python
import argparse
import os
import sys

from cosmos.api import add_execution_args, Execution
from genomekey.util.misc import assert_references_exist
from genomekey.api import library_path, s3run, s3cmd
from genomekey.aws.config import print_env_export
from genomekey import environment

opj = os.path.join

env = None
# def do_growl(msg, hostname=os.environ.get('SSH_CLIENT', '').split(' ')[0]):
# from cosmos.util import growl
#
#     growl.send(msg, hostname=hostname)
#
#
# def text_on_finish(ex):
#     from cosmos import signal_execution_status_change, ExecutionStatus
#
#     @signal_execution_status_change.connect
#     def sig(ex):
#         msg = "%s %s" % (ex, ex.status)
#         if ex.status in [ExecutionStatus.successful, ExecutionStatus.failed]:
#             text_message(msg)
#             ex.log.info('Sent a text message')
#
#     def text_message(message):
#         from twilio.rest import TwilioRestClient
#
#         account = s['gk']['twilio_account']
#         token = s['gk']['twilio_token']
#         phone_number = "+1%s" % s['gk']['phone_number']
#         client = TwilioRestClient(account, token)
#
#         message = client.messages.create(to=phone_number, from_='+15203086165', body=message)

def germline(*args, **kwargs):
    # This function exists so that importing run_germline doesn't import the environment before it's initialized
    from genomekey.workflows.germline.recipe import run_germline
    run_germline(*args, **kwargs)

def initdb():
    env.cosmos_app.initdb()

def resetdb():
    env.cosmos_app.resetdb()

def shell():
    env.cosmos_app.shell()

def runweb(host, port):
    env.cosmos_app.runweb(host,port)

def parse_args():

    parser = argparse.ArgumentParser()
    parser.add_argument('--ipdb', '-d',
                        help='Launch ipdb on exception',
                        action='store_true')
    parser.add_argument('--config_path', help='genomekey.conf', default=opj(library_path, 'etc/genomekey.conf'))
    parser.add_argument('-v', '--reference_version', choices=('hg38', 'b37'), default='b37')

    sps = parser.add_subparsers(title="Commands", metavar="<command>", help='commands')

    sp = sps.add_parser('resetdb', help=resetdb.__doc__)
    sp.set_defaults(func=resetdb)

    sp = sps.add_parser('aws', help='print export commands for AWS creentials')
    sp.set_defaults(func=print_env_export)

    sp = sps.add_parser('initdb', help=initdb.__doc__)
    sp.set_defaults(func=initdb)

    sp = sps.add_parser('shell', help=shell.__doc__)
    sp.set_defaults(func=shell)

    sp = sps.add_parser('runweb', help=runweb.__doc__)
    sp.add_argument('-p', '--port', type=int, default=5000, help='port to bind the server to')
    sp.add_argument('-H', '--host', default='0.0.0.0', help='host to bind the server to')
    sp.set_defaults(func=runweb)


    def add_execution_args2(sp):
        add_execution_args(sp)
        sp.add_argument('-m', '--comments', help="adds comments to the execution's info field")
        sp.add_argument('-drm', '--default_drm', help="overrides config value")
        # sp.add_argument('-s', '--sms', help='send an sms after an execution finishes with its status',
        #                 action='store_true')
        # sp.add_argument('-g', '--growl', help='sends growl notifications on execution status changes',
        #                 action='store_true')


    sp = sps.add_parser('germline', help=germline.__doc__)
    sp.set_defaults(func=germline)
    sp.add_argument('input_path', help='file with inputs')
    # sp.add_argument('--s3fs', '-s3', default=False,
    #                 help='EXPERIMENTAL.  upload/download all output files to this s3 bucket.  Removes the need'
    #                      'for a shared filesystem')
    sp.add_argument('--target_bed', '-t', required=True)  # default to whole genome target bed?
    add_execution_args2(sp)

    # sp = sps.add_parser('growl', help='send a message over growl')
    # sp.add_argument('msg', help='message to send')
    # sp.add_argument('-H', '--hostname')
    # sp.set_defaults(func=do_growl)

    args = parser.parse_args()
    kwargs = dict(args._get_kwargs())
    f = kwargs.pop('func')

    config_path = kwargs.pop('config_path')
    global env
    env = environment.initialize(config_path, reference_version=kwargs.pop('reference_version'))

    workflow_fxns = dict(germline='Germline')



    # Todo refactor this into functions
    if f.__name__ in workflow_fxns.keys():

        # assert_references_exist()
        d = {n: kwargs.pop(n) for n in ['name', 'restart', 'skip_confirm', 'comments']}
        comments = d.pop('comments')
        workflow_type = workflow_fxns[f.__name__]

        def get_output_dir():
            root_output_dir = opj(env.config['gk']['analysis_output'], workflow_type)
            os.system('mkdir -p %s' % root_output_dir)
            return opj(root_output_dir, d['name'])

        d['output_dir'] = get_output_dir()

        resuming_execution = env.cosmos_app.session.query(Execution).filter_by(name=d['name']).count() == 1

        # if d['restart'] and os.path.exists(d['output_dir']):
        #     import shutil
        #     shutil.rmtree(d['output_dir'])

        ex = env.cosmos_app.start(**d)

        ex.info['workflow_type'] = workflow_type
        ex.info['comments'] = comments
        kwargs['execution'] = ex

        # s3fs = kwargs.get('s3fs', None)
        # if s3fs:
        #     # AWS cli will produce the following error sometimes if this environment variable is not set
        #     # [Errno 1] _ssl.c:504: error:14090086:SSL routines:SSL3_GET_SERVER_CERTIFICATE:certificate verify failed
        #     os.environ['REQUESTS_CA_BUNDLE'] = '/etc/ssl/certs/ca-certificates.crt'
        #
        #     s3fs = os.path.join(s3fs, d['name'])
        #     kwargs['s3fs'] = s3fs
        #     ex.info['s3fs'] = s3fs
        #
        #     dir_exists = s3run.path_exists(s3fs)
        #     if dir_exists:
        #         if not resuming_execution:
        #             ex.log.error('Cannot write to %s, the directory already exists but Execution with name `%s` was not in the database.  '
        #                          'Either use a different execution name, or run: \n`aws s3 rm %s --recursive`' % (s3fs, d['name'], s3fs))
        #             ex.terminate(due_to_failure=False)
        #             ex.delete(delete_files=True)
        #             sys.exit(0)
        #         elif d['restart']:
        #             ex.log.info('Deleting S3 Objects in %s since restart==True' % s3fs)
        #             s3run.delete_s3_dir(s3fs, skip_confirm=d['skip_confirm'])



        # required for gof3r, which doesn't check ~/.aws/config files
        # gof3r is used to stream input files from s3 for some tools
        # from genomekey.aws.config import set_env_aws_credentials
        #
        # set_env_aws_credentials()

    # Send growl on fail
    if kwargs.pop('growl', False):
        from cosmos import signal_execution_status_change, ExecutionStatus

        @signal_execution_status_change.connect
        def growl_signal(execution):
            if execution.status not in [ExecutionStatus.running, ExecutionStatus.killed]:
                do_growl('%s %s' % (execution, execution.status))

    # Send a text on execution end
    if kwargs.pop('sms', False):
        text_on_finish(ex)

    # overwrite default_drm in conf?
    default_drm = kwargs.pop('default_drm', None)
    if default_drm:
        env.cosmos_app.default_drm = default_drm

    # launch ipdb on exception if --ipdb
    if kwargs.pop('ipdb', False):
        import ipdb

        with ipdb.launch_ipdb_on_exception():
            f(**kwargs)
    else:
        f(**kwargs)

    if 'execution' in kwargs:
        exit_code = 0 if kwargs['execution'].successful else 1
        sys.exit(exit_code)


if __name__ == '__main__':
    parse_args()